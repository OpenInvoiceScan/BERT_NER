{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForTokenClassification, BertTokenizerFast\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LabelEncoder<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.LabelEncoder.html\">?<span>Documentation for LabelEncoder</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LabelEncoder()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = 'google-bert/bert-base-multilingual-cased'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "label_encoder = LabelEncoder()\n",
    "labels = [\n",
    "    \"O\",  # Para tokens que no son parte de ninguna entidad nombrada\n",
    "    \"B-invoice_id\", \"I-invoice_id\",\n",
    "    \"B-issue_date\", \"I-issue_date\",\n",
    "    \"B-due_date\", \"I-due_date\",\n",
    "    \"B-issuer_name\", \"I-issuer_name\",\n",
    "    \"B-issuer_address\", \"I-issuer_address\",\n",
    "    \"B-issuer_phone\",\n",
    "    \"B-issuer_email\",\n",
    "    \"B-issuer_tax_id\",\n",
    "    \"B-recipient_name\", \"I-recipient_name\",\n",
    "    \"B-recipient_address\", \"I-recipient_address\",\n",
    "    \"B-recipient_phone\",\n",
    "    \"B-recipient_email\",\n",
    "    \"B-recipient_tax_id\",\n",
    "    \"B-item_description\", \"I-item_description\",\n",
    "    \"B-item_quantity\",\n",
    "    \"B-item_unit_price\",\n",
    "    \"B-item_total\",\n",
    "    \"B-subtotal\",\n",
    "    \"B-tax_description\", \"I-tax_description\",\n",
    "    \"B-tax_percentage\",\n",
    "    \"B-tax_amount\",\n",
    "    \"B-total\",\n",
    "    \"B-payment_method\",\n",
    "    \"UNK\"\n",
    "]\n",
    "\n",
    "label_encoder.fit(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_tokens_and_labels(text,tags):\n",
    "    encoded_input = tokenizer(text, is_split_into_words=True, padding=\"max_length\", truncation=True, max_length=512)\n",
    "    encoded_as_text = tokenizer.convert_ids_to_tokens(encoded_input[\"input_ids\"])\n",
    "\n",
    "    word_ids = encoded_input.word_ids()\n",
    "\n",
    "    labels = []\n",
    "    for i in range(len(word_ids)):\n",
    "        if word_ids[i] is None:\n",
    "            labels.append('UNK')\n",
    "        else:\n",
    "            labels.append(tags[word_ids[i]])\n",
    "    \n",
    "    labels = label_encoder.transform(labels)\n",
    "    \n",
    "    return {\n",
    "        \"encoded_input\": encoded_input, \n",
    "        \"encoded_labels\": labels,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "class InvoiceDataset(Dataset):\n",
    "    def __init__(self, texts, tags, max_len=512):\n",
    "        self.texts = texts\n",
    "        self.tags = tags\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        tags = self.tags[idx]\n",
    "\n",
    "\n",
    "        # Tokenización y alineación de etiquetas\n",
    "        result = align_tokens_and_labels(text, tags)\n",
    "        encoding = result[\"encoded_input\"]\n",
    "        labels = result[\"encoded_labels\"]\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(encoding['input_ids'], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(encoding['attention_mask'], dtype=torch.long),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-invoice_id', 'O', 'O', 'O', 'O', 'B-issue_date', 'O', 'O', 'O', 'O', 'B-due_date', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-issuer_name', 'I-issuer_name', 'B-recipient_name', 'I-recipient_name', 'B-issuer_address', 'I-issuer_address', 'I-issuer_address', 'I-issuer_address', 'I-issuer_address', 'I-issuer_address', 'I-issuer_address', 'I-issuer_address', 'I-issuer_address', 'B-recipient_address', 'I-recipient_address', 'I-recipient_address', 'I-recipient_address', 'I-recipient_address', 'I-recipient_address', 'B-issuer_phone', 'B-recipient_phone', 'B-issuer_email', 'B-recipient_email', 'B-issuer_tax_id', 'B-recipient_tax_id', 'O', 'O', 'O', 'O', 'O', 'B-item_description', 'I-item_description', 'I-item_description', 'B-item_quantity', 'B-item_unit_price', 'B-item_total', 'B-item_description', 'I-item_description', 'I-item_description', 'B-item_quantity', 'B-item_unit_price', 'B-item_total', 'O', 'O', 'B-subtotal', 'B-tax_description', 'B-tax_percentage', 'O', 'B-tax_amount', 'O', 'O', 'B-total', 'O', 'O', 'O', 'O', 'B-payment_method']\n",
      "[32  1 32 32 32 32  2 32 32 32 32  0 32 32 32 32 32 32 32 32  5 27 15 30\n",
      "  3 26 26 26 26 26 26 26 26 13 29 29 29 29 29  6 16  4 14  7 17 32 32 32\n",
      " 32 32  8 28 28  9 11 10  8 28 28  9 11 10 32 32 18 20 21 32 19 32 32 22\n",
      " 32 32 32 32 12]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "texts = []  # Lista de textos de factura\n",
    "tags = []  # Lista de etiquetas (cada etiqueta es una lista de ids de etiquetas)\n",
    "\n",
    "def load_tags(file_path):\n",
    "    tags = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Dividir la línea por ' -> ' y tomar el segundo elemento, que es la etiqueta\n",
    "            parts = line.strip().split(' -> ')\n",
    "            if len(parts) > 1:\n",
    "                tags.append(parts[1])  # Agrega la etiqueta a la lista\n",
    "    return tags\n",
    "\n",
    "def load_text(file_path):\n",
    "    texts = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Dividir la línea por ' -> ' y tomar el segundo elemento, que es la etiqueta\n",
    "            parts = line.strip().split(' -> ')\n",
    "            if len(parts) > 1:\n",
    "                texts.append(parts[0])  # Agrega la etiqueta a la lista\n",
    "    return texts\n",
    "\n",
    "tags = []\n",
    "for i in range(10):  # Ajusta el rango según la cantidad de facturas\n",
    "    tags.append(load_tags(f'facturas/factura{i}.tokens'))\n",
    "    texts.append(load_text(f'facturas/factura{i}.tokens'))\n",
    "\n",
    "# Crear el dataset y dataloader\n",
    "dataset = InvoiceDataset(texts, tags)\n",
    "loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "print(tags[0])\n",
    "print(label_encoder.transform(tags[0]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 85245, 11465, 143, 32792, 22650, 11396, 11305, 20187, 12964, 10104, 83054, 131, 22171, 11011, 118, 10814, 118, 10907, 20187, 12964, 10104, 26044, 43053, 131, 22171, 11011, 118, 10831, 118, 10814, 84387, 10127, 11289, 105580, 131, 84387, 10127, 97200, 54973, 10667, 131, 73230, 15129, 26580, 43707, 13820, 27173, 67574, 15377, 34528, 138, 14971, 119, 56114, 11830, 10900, 117, 80821, 49517, 20305, 11383, 11396, 13759, 90847, 25675, 23044, 117, 25067, 10878, 89478, 70072, 118, 56267, 118, 40633, 10929, 10686, 21069, 49469, 11396, 17449, 118, 33195, 118, 69717, 118, 29718, 11305, 29467, 10874, 40762, 19029, 137, 77436, 119, 26978, 20169, 10237, 76977, 137, 29698, 85505, 119, 10212, 193, 38850, 10929, 103450, 13695, 11703, 99555, 10884, 39999, 10162, 32168, 11166, 10858, 12387, 11373, 68430, 13810, 51991, 14820, 26680, 10162, 35248, 13584, 19919, 16780, 25220, 11639, 118, 30798, 47543, 10870, 11356, 118, 13596, 29277, 10107, 122, 12074, 119, 12535, 12074, 119, 12535, 79326, 25470, 104071, 22414, 17788, 10107, 126, 10150, 119, 11035, 10462, 119, 10827, 24358, 40530, 10415, 131, 19615, 119, 10270, 69342, 11090, 113, 10250, 110, 114, 131, 10306, 119, 10365, 25220, 131, 21163, 119, 11528, 150, 13837, 43329, 10104, 74004, 131, 75181, 38040, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[33, 32, 32, 1, 1, 1, 1, 1, 32, 32, 32, 32, 32, 2, 2, 2, 2, 2, 2, 32, 32, 32, 32, 32, 32, 0, 0, 0, 0, 0, 0, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 5, 27, 15, 30, 30, 3, 3, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 13, 13, 29, 29, 29, 29, 29, 29, 29, 29, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 16, 16, 16, 16, 16, 16, 16, 16, 4, 4, 4, 4, 4, 4, 4, 4, 14, 14, 14, 14, 14, 14, 14, 14, 7, 7, 7, 7, 7, 7, 7, 7, 17, 17, 17, 17, 17, 17, 17, 17, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 8, 8, 8, 8, 8, 28, 28, 28, 28, 28, 9, 11, 11, 11, 10, 10, 10, 8, 28, 28, 28, 28, 28, 9, 11, 11, 11, 10, 10, 10, 32, 32, 32, 32, 18, 18, 18, 20, 20, 21, 21, 21, 21, 32, 19, 19, 19, 32, 32, 22, 22, 22, 32, 32, 32, 32, 32, 32, 12, 12, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33]\n",
      "[CLS] ['UNK']\n",
      "Fact ['O']\n",
      "##ura ['O']\n",
      "F ['B-invoice_id']\n",
      "##30 ['B-invoice_id']\n",
      "##20 ['B-invoice_id']\n",
      "##8 ['B-invoice_id']\n",
      "##7 ['B-invoice_id']\n",
      "Fe ['O']\n",
      "##cha ['O']\n",
      "de ['O']\n",
      "emisión ['O']\n",
      ": ['O']\n",
      "202 ['B-issue_date']\n",
      "##4 ['B-issue_date']\n",
      "- ['B-issue_date']\n",
      "04 ['B-issue_date']\n",
      "- ['B-issue_date']\n",
      "03 ['B-issue_date']\n",
      "Fe ['O']\n",
      "##cha ['O']\n",
      "de ['O']\n",
      "ven ['O']\n",
      "##cimiento ['O']\n",
      ": ['O']\n",
      "202 ['B-due_date']\n",
      "##4 ['B-due_date']\n",
      "- ['B-due_date']\n",
      "05 ['B-due_date']\n",
      "- ['B-due_date']\n",
      "04 ['B-due_date']\n",
      "Datos ['O']\n",
      "del ['O']\n",
      "Em ['O']\n",
      "##isor ['O']\n",
      ": ['O']\n",
      "Datos ['O']\n",
      "del ['O']\n",
      "Rec ['O']\n",
      "##ept ['O']\n",
      "##or ['O']\n",
      ": ['O']\n",
      "Escobar ['B-issuer_name']\n",
      "Ltd ['I-issuer_name']\n",
      "Dawn ['B-recipient_name']\n",
      "Hu ['I-recipient_name']\n",
      "##ff ['I-recipient_name']\n",
      "237 ['B-issuer_address']\n",
      "##53 ['B-issuer_address']\n",
      "Kevin ['I-issuer_address']\n",
      "Heights ['I-issuer_address']\n",
      "A ['I-issuer_address']\n",
      "##pt ['I-issuer_address']\n",
      ". ['I-issuer_address']\n",
      "952 ['I-issuer_address']\n",
      "East ['I-issuer_address']\n",
      "James ['I-issuer_address']\n",
      ", ['I-issuer_address']\n",
      "NH ['I-issuer_address']\n",
      "581 ['I-issuer_address']\n",
      "##10 ['I-issuer_address']\n",
      "1931 ['B-recipient_address']\n",
      "##8 ['B-recipient_address']\n",
      "Lewis ['I-recipient_address']\n",
      "Alley ['I-recipient_address']\n",
      "Jessica ['I-recipient_address']\n",
      "##fort ['I-recipient_address']\n",
      ", ['I-recipient_address']\n",
      "DE ['I-recipient_address']\n",
      "07 ['I-recipient_address']\n",
      "##200 ['I-recipient_address']\n",
      "852 ['B-issuer_phone']\n",
      "- ['B-issuer_phone']\n",
      "611 ['B-issuer_phone']\n",
      "- ['B-issuer_phone']\n",
      "760 ['B-issuer_phone']\n",
      "##0 ['B-issuer_phone']\n",
      "##x ['B-issuer_phone']\n",
      "##00 ['B-issuer_phone']\n",
      "##43 ['B-issuer_phone']\n",
      "##8 ['B-issuer_phone']\n",
      "001 ['B-recipient_phone']\n",
      "- ['B-recipient_phone']\n",
      "355 ['B-recipient_phone']\n",
      "- ['B-recipient_phone']\n",
      "759 ['B-recipient_phone']\n",
      "- ['B-recipient_phone']\n",
      "321 ['B-recipient_phone']\n",
      "##7 ['B-recipient_phone']\n",
      "rose ['B-issuer_email']\n",
      "##w ['B-issuer_email']\n",
      "##hit ['B-issuer_email']\n",
      "##ney ['B-issuer_email']\n",
      "@ ['B-issuer_email']\n",
      "hull ['B-issuer_email']\n",
      ". ['B-issuer_email']\n",
      "info ['B-issuer_email']\n",
      "lea ['B-recipient_email']\n",
      "##h ['B-recipient_email']\n",
      "##45 ['B-recipient_email']\n",
      "@ ['B-recipient_email']\n",
      "hot ['B-recipient_email']\n",
      "##mail ['B-recipient_email']\n",
      ". ['B-recipient_email']\n",
      "com ['B-recipient_email']\n",
      "y ['B-issuer_tax_id']\n",
      "##IL ['B-issuer_tax_id']\n",
      "##0 ['B-issuer_tax_id']\n",
      "##86 ['B-issuer_tax_id']\n",
      "##tz ['B-issuer_tax_id']\n",
      "##q ['B-issuer_tax_id']\n",
      "##55 ['B-issuer_tax_id']\n",
      "##3 ['B-issuer_tax_id']\n",
      "BA ['B-recipient_tax_id']\n",
      "##d ['B-recipient_tax_id']\n",
      "##48 ['B-recipient_tax_id']\n",
      "##5 ['B-recipient_tax_id']\n",
      "##C ['B-recipient_tax_id']\n",
      "##ms ['B-recipient_tax_id']\n",
      "##9 ['B-recipient_tax_id']\n",
      "##32 ['B-recipient_tax_id']\n",
      "Des ['O']\n",
      "##cripción ['O']\n",
      "Can ['O']\n",
      "##tida ['O']\n",
      "##d ['O']\n",
      "Pre ['O']\n",
      "##cio ['O']\n",
      "Unit ['O']\n",
      "##ario ['O']\n",
      "Total ['O']\n",
      "re ['B-item_description']\n",
      "- ['B-item_description']\n",
      "context ['B-item_description']\n",
      "##uali ['B-item_description']\n",
      "##ze ['B-item_description']\n",
      "world ['I-item_description']\n",
      "- ['I-item_description']\n",
      "class ['I-item_description']\n",
      "portal ['I-item_description']\n",
      "##s ['I-item_description']\n",
      "1 ['B-item_quantity']\n",
      "88 ['B-item_unit_price']\n",
      ". ['B-item_unit_price']\n",
      "74 ['B-item_unit_price']\n",
      "88 ['B-item_total']\n",
      ". ['B-item_total']\n",
      "74 ['B-item_total']\n",
      "enable ['B-item_description']\n",
      "ro ['I-item_description']\n",
      "##bust ['I-item_description']\n",
      "method ['I-item_description']\n",
      "##ologie ['I-item_description']\n",
      "##s ['I-item_description']\n",
      "5 ['B-item_quantity']\n",
      "10 ['B-item_unit_price']\n",
      ". ['B-item_unit_price']\n",
      "09 ['B-item_unit_price']\n",
      "50 ['B-item_total']\n",
      ". ['B-item_total']\n",
      "45 ['B-item_total']\n",
      "Sub ['O']\n",
      "##tot ['O']\n",
      "##al ['O']\n",
      ": ['O']\n",
      "139 ['B-subtotal']\n",
      ". ['B-subtotal']\n",
      "19 ['B-subtotal']\n",
      "VA ['B-tax_description']\n",
      "##T ['B-tax_description']\n",
      "( ['B-tax_percentage']\n",
      "16 ['B-tax_percentage']\n",
      "% ['B-tax_percentage']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ") ['B-tax_percentage']\n",
      ": ['O']\n",
      "22 ['B-tax_amount']\n",
      ". ['B-tax_amount']\n",
      "27 ['B-tax_amount']\n",
      "Total ['O']\n",
      ": ['O']\n",
      "161 ['B-total']\n",
      ". ['B-total']\n",
      "46 ['B-total']\n",
      "M ['O']\n",
      "##ét ['O']\n",
      "##odo ['O']\n",
      "de ['O']\n",
      "pago ['O']\n",
      ": ['O']\n",
      "Credit ['B-payment_method']\n",
      "Card ['B-payment_method']\n",
      "[SEP] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n",
      "[PAD] ['UNK']\n"
     ]
    }
   ],
   "source": [
    "#Probar a tokenizar un texto para ver su longitud tokenizada\n",
    "\n",
    "print(dataset[0]['input_ids'].tolist())\n",
    "print(dataset[0]['labels'].tolist())\n",
    "\n",
    "ids = tokenizer.convert_ids_to_tokens(dataset[0]['input_ids'])\n",
    "\n",
    "for i in range(len(ids)):\n",
    "    print(ids[i], label_encoder.inverse_transform([dataset[0]['labels'][i].item()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Dividir datos\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, tags, test_size=0.1)\n",
    "\n",
    "# Crear DataLoaders\n",
    "train_dataset = InvoiceDataset(train_texts, train_labels)\n",
    "val_dataset = InvoiceDataset(val_texts, val_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Determinar el dispositivo a usar (GPU si está disponible, de lo contrario CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Modelo\n",
    "model = BertForTokenClassification.from_pretrained(MODEL_NAME, num_labels=len(labels))\n",
    "model.to(device)  # Mover el modelo a la GPU si está disponible\n",
    "\n",
    "# Optimizador\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Función de entrenamiento\n",
    "def train(model, dataloader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        # Mover los datos al dispositivo correcto\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. GPU: NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Debugging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 3.2029229402542114\n",
      "Epoch 2, Train Loss: 1.9260507225990295\n",
      "Epoch 3, Train Loss: 1.7057088017463684\n",
      "Epoch 4, Train Loss: 1.6786693930625916\n",
      "Epoch 5, Train Loss: 1.312235713005066\n",
      "Epoch 6, Train Loss: 1.0991789102554321\n",
      "Epoch 7, Train Loss: 0.9233507812023163\n",
      "Epoch 8, Train Loss: 0.925711452960968\n",
      "Epoch 9, Train Loss: 0.8094072639942169\n",
      "Epoch 10, Train Loss: 0.7061797082424164\n",
      "Epoch 11, Train Loss: 0.5781725645065308\n",
      "Epoch 12, Train Loss: 0.508179783821106\n",
      "Epoch 13, Train Loss: 0.4347299784421921\n",
      "Epoch 14, Train Loss: 0.37733136117458344\n",
      "Epoch 15, Train Loss: 0.36994677782058716\n",
      "Epoch 16, Train Loss: 0.3232783377170563\n",
      "Epoch 17, Train Loss: 0.32145826518535614\n",
      "Epoch 18, Train Loss: 0.2721906155347824\n",
      "Epoch 19, Train Loss: 0.2419148087501526\n",
      "Epoch 20, Train Loss: 0.200696662068367\n",
      "Epoch 21, Train Loss: 0.21911227703094482\n",
      "Epoch 22, Train Loss: 0.16867458075284958\n",
      "Epoch 23, Train Loss: 0.16425105184316635\n",
      "Epoch 24, Train Loss: 0.14910607039928436\n",
      "Epoch 25, Train Loss: 0.15139005333185196\n",
      "Epoch 26, Train Loss: 0.1399495080113411\n",
      "Epoch 27, Train Loss: 0.12326789274811745\n",
      "Epoch 28, Train Loss: 0.10376360267400742\n",
      "Epoch 29, Train Loss: 0.10659869015216827\n",
      "Epoch 30, Train Loss: 0.08897943422198296\n",
      "Epoch 31, Train Loss: 0.0809766948223114\n",
      "Epoch 32, Train Loss: 0.07429590821266174\n",
      "Epoch 33, Train Loss: 0.06557168066501617\n",
      "Epoch 34, Train Loss: 0.06428447365760803\n",
      "Epoch 35, Train Loss: 0.05289655737578869\n",
      "Epoch 36, Train Loss: 0.051923997700214386\n",
      "Epoch 37, Train Loss: 0.04889490641653538\n",
      "Epoch 38, Train Loss: 0.047253306955099106\n",
      "Epoch 39, Train Loss: 0.040621303021907806\n",
      "Epoch 40, Train Loss: 0.03818747587502003\n",
      "Epoch 41, Train Loss: 0.037859465926885605\n",
      "Epoch 42, Train Loss: 0.032795664854347706\n",
      "Epoch 43, Train Loss: 0.03321567736566067\n",
      "Epoch 44, Train Loss: 0.028338806703686714\n",
      "Epoch 45, Train Loss: 0.02990535367280245\n",
      "Epoch 46, Train Loss: 0.026936586014926434\n",
      "Epoch 47, Train Loss: 0.025133556686341763\n",
      "Epoch 48, Train Loss: 0.02626525331288576\n",
      "Epoch 49, Train Loss: 0.023407431319355965\n",
      "Epoch 50, Train Loss: 0.023072046227753162\n",
      "Epoch 51, Train Loss: 0.024925529956817627\n",
      "Epoch 52, Train Loss: 0.021473512053489685\n",
      "Epoch 53, Train Loss: 0.020732061006128788\n",
      "Epoch 54, Train Loss: 0.019913451746106148\n",
      "Epoch 55, Train Loss: 0.026329265907406807\n",
      "Epoch 56, Train Loss: 0.017880547791719437\n",
      "Epoch 57, Train Loss: 0.0200913455337286\n",
      "Epoch 58, Train Loss: 0.02099843230098486\n",
      "Epoch 59, Train Loss: 0.018859630450606346\n",
      "Epoch 60, Train Loss: 0.01681222114712\n",
      "Epoch 61, Train Loss: 0.016643045470118523\n",
      "Epoch 62, Train Loss: 0.017100361175835133\n",
      "Epoch 63, Train Loss: 0.015213024336844683\n",
      "Epoch 64, Train Loss: 0.014495967421680689\n",
      "Epoch 65, Train Loss: 0.014335839543491602\n",
      "Epoch 66, Train Loss: 0.013703894801437855\n",
      "Epoch 67, Train Loss: 0.012854641769081354\n",
      "Epoch 68, Train Loss: 0.013079894706606865\n",
      "Epoch 69, Train Loss: 0.011879263445734978\n",
      "Epoch 70, Train Loss: 0.011827622540295124\n",
      "Epoch 71, Train Loss: 0.011623626574873924\n",
      "Epoch 72, Train Loss: 0.011162322480231524\n",
      "Epoch 73, Train Loss: 0.01186687359586358\n",
      "Epoch 74, Train Loss: 0.010118136648088694\n",
      "Epoch 75, Train Loss: 0.014987147878855467\n",
      "Epoch 76, Train Loss: 0.013313098344951868\n",
      "Epoch 77, Train Loss: 0.014876841101795435\n",
      "Epoch 78, Train Loss: 0.012091561686247587\n",
      "Epoch 79, Train Loss: 0.014682549983263016\n",
      "Epoch 80, Train Loss: 0.010704359970986843\n",
      "Epoch 81, Train Loss: 0.010637357365339994\n",
      "Epoch 82, Train Loss: 0.011497246567159891\n",
      "Epoch 83, Train Loss: 0.010221375152468681\n",
      "Epoch 84, Train Loss: 0.014043417759239674\n",
      "Epoch 85, Train Loss: 0.009469051379710436\n",
      "Epoch 86, Train Loss: 0.013675709255039692\n",
      "Epoch 87, Train Loss: 0.009980030823498964\n",
      "Epoch 88, Train Loss: 0.00898845074698329\n",
      "Epoch 89, Train Loss: 0.0089480085298419\n",
      "Epoch 90, Train Loss: 0.008975895121693611\n",
      "Epoch 91, Train Loss: 0.008794889319688082\n",
      "Epoch 92, Train Loss: 0.008632080163806677\n",
      "Epoch 93, Train Loss: 0.008907574461773038\n",
      "Epoch 94, Train Loss: 0.007894628681242466\n",
      "Epoch 95, Train Loss: 0.007490034447982907\n",
      "Epoch 96, Train Loss: 0.007388702128082514\n",
      "Epoch 97, Train Loss: 0.007374532055109739\n",
      "Epoch 98, Train Loss: 0.007520391372963786\n",
      "Epoch 99, Train Loss: 0.007282551610842347\n",
      "Epoch 100, Train Loss: 0.006929506314918399\n",
      "Epoch 101, Train Loss: 0.00644229375757277\n",
      "Epoch 102, Train Loss: 0.006311630364507437\n",
      "Epoch 103, Train Loss: 0.006243706680834293\n",
      "Epoch 104, Train Loss: 0.006401060149073601\n",
      "Epoch 105, Train Loss: 0.006770794512704015\n",
      "Epoch 106, Train Loss: 0.00587586872279644\n",
      "Epoch 107, Train Loss: 0.005320718977600336\n",
      "Epoch 108, Train Loss: 0.0055806071031838655\n",
      "Epoch 109, Train Loss: 0.005170635413378477\n",
      "Epoch 110, Train Loss: 0.005260774865746498\n",
      "Epoch 111, Train Loss: 0.005259312689304352\n",
      "Epoch 112, Train Loss: 0.00494164670817554\n",
      "Epoch 113, Train Loss: 0.005095761734992266\n",
      "Epoch 114, Train Loss: 0.004768066108226776\n",
      "Epoch 115, Train Loss: 0.004841132555156946\n",
      "Epoch 116, Train Loss: 0.00470480602234602\n",
      "Epoch 117, Train Loss: 0.004498067777603865\n",
      "Epoch 118, Train Loss: 0.004419035976752639\n",
      "Epoch 119, Train Loss: 0.004731570370495319\n",
      "Epoch 120, Train Loss: 0.004844271810725331\n",
      "Epoch 121, Train Loss: 0.004369482398033142\n",
      "Epoch 122, Train Loss: 0.004424165701493621\n",
      "Epoch 123, Train Loss: 0.004962974227964878\n",
      "Epoch 124, Train Loss: 0.0042959090787917376\n",
      "Epoch 125, Train Loss: 0.0039857999654486775\n",
      "Epoch 126, Train Loss: 0.00396964221727103\n",
      "Epoch 127, Train Loss: 0.004217952722683549\n",
      "Epoch 128, Train Loss: 0.00408644531853497\n",
      "Epoch 129, Train Loss: 0.004030344309285283\n",
      "Epoch 130, Train Loss: 0.004423933569341898\n",
      "Epoch 131, Train Loss: 0.0037118811160326004\n",
      "Epoch 132, Train Loss: 0.003960217931307852\n",
      "Epoch 133, Train Loss: 0.003625878016464412\n",
      "Epoch 134, Train Loss: 0.0036963956663385034\n",
      "Epoch 135, Train Loss: 0.0035727767972275615\n",
      "Epoch 136, Train Loss: 0.0037058020243421197\n",
      "Epoch 137, Train Loss: 0.0034993833396583796\n",
      "Epoch 138, Train Loss: 0.003632454201579094\n",
      "Epoch 139, Train Loss: 0.003604562021791935\n",
      "Epoch 140, Train Loss: 0.0036068428307771683\n",
      "Epoch 141, Train Loss: 0.003375206491909921\n",
      "Epoch 142, Train Loss: 0.003373132785782218\n",
      "Epoch 143, Train Loss: 0.0032496557105332613\n",
      "Epoch 144, Train Loss: 0.003299467614851892\n",
      "Epoch 145, Train Loss: 0.0034521528286859393\n",
      "Epoch 146, Train Loss: 0.003226236207410693\n",
      "Epoch 147, Train Loss: 0.0031844706973060966\n",
      "Epoch 148, Train Loss: 0.0031741365091875196\n",
      "Epoch 149, Train Loss: 0.0030594990821555257\n",
      "Epoch 150, Train Loss: 0.003062778851017356\n",
      "Epoch 151, Train Loss: 0.0032495229970663786\n",
      "Epoch 152, Train Loss: 0.003070578328333795\n",
      "Epoch 153, Train Loss: 0.003950977697968483\n",
      "Epoch 154, Train Loss: 0.0030084946192801\n",
      "Epoch 155, Train Loss: 0.0028911943081766367\n",
      "Epoch 156, Train Loss: 0.0030679526971653104\n",
      "Epoch 157, Train Loss: 0.003026021411642432\n",
      "Epoch 158, Train Loss: 0.002848173025995493\n",
      "Epoch 159, Train Loss: 0.0029721329919993877\n",
      "Epoch 160, Train Loss: 0.0029248306527733803\n",
      "Epoch 161, Train Loss: 0.002896622405387461\n",
      "Epoch 162, Train Loss: 0.002922798856161535\n",
      "Epoch 163, Train Loss: 0.002754705143161118\n",
      "Epoch 164, Train Loss: 0.0028596275951713324\n",
      "Epoch 165, Train Loss: 0.002759238937869668\n",
      "Epoch 166, Train Loss: 0.002790527301840484\n",
      "Epoch 167, Train Loss: 0.0028286351589486003\n",
      "Epoch 168, Train Loss: 0.0027451820205897093\n",
      "Epoch 169, Train Loss: 0.00267005932983011\n",
      "Epoch 170, Train Loss: 0.0027732569724321365\n",
      "Epoch 171, Train Loss: 0.0025979032507166266\n",
      "Epoch 172, Train Loss: 0.0025131204165518284\n",
      "Epoch 173, Train Loss: 0.0026547412853688\n",
      "Epoch 174, Train Loss: 0.002641805331222713\n",
      "Epoch 175, Train Loss: 0.002602357300929725\n",
      "Epoch 176, Train Loss: 0.0026205803733319044\n",
      "Epoch 177, Train Loss: 0.0024767761351540685\n",
      "Epoch 178, Train Loss: 0.0025453364942222834\n",
      "Epoch 179, Train Loss: 0.002542936592362821\n",
      "Epoch 180, Train Loss: 0.0025054593570530415\n",
      "Epoch 181, Train Loss: 0.002497053239494562\n",
      "Epoch 182, Train Loss: 0.0024554621195420623\n",
      "Epoch 183, Train Loss: 0.0023934971541166306\n",
      "Epoch 184, Train Loss: 0.002416245872154832\n",
      "Epoch 185, Train Loss: 0.003280287957750261\n",
      "Epoch 186, Train Loss: 0.002438121009618044\n",
      "Epoch 187, Train Loss: 0.0024097549030557275\n",
      "Epoch 188, Train Loss: 0.0023640182334929705\n",
      "Epoch 189, Train Loss: 0.0023354138247668743\n",
      "Epoch 190, Train Loss: 0.002410507877357304\n",
      "Epoch 191, Train Loss: 0.002260562148876488\n",
      "Epoch 192, Train Loss: 0.0022595355985686183\n",
      "Epoch 193, Train Loss: 0.002250216086395085\n",
      "Epoch 194, Train Loss: 0.002315101446583867\n",
      "Epoch 195, Train Loss: 0.0033655232982710004\n",
      "Epoch 196, Train Loss: 0.002260579727590084\n",
      "Epoch 197, Train Loss: 0.0022907464299350977\n",
      "Epoch 198, Train Loss: 0.0022403651382774115\n",
      "Epoch 199, Train Loss: 0.0021674740128219128\n",
      "Epoch 200, Train Loss: 0.0021638853941112757\n",
      "Epoch 201, Train Loss: 0.0022823645267635584\n",
      "Epoch 202, Train Loss: 0.0022448465460911393\n",
      "Epoch 203, Train Loss: 0.0022092930739745498\n",
      "Epoch 204, Train Loss: 0.0021194262662902474\n",
      "Epoch 205, Train Loss: 0.002130195265635848\n",
      "Epoch 206, Train Loss: 0.0021829967154189944\n",
      "Epoch 207, Train Loss: 0.0021111557725816965\n",
      "Epoch 208, Train Loss: 0.0022097874898463488\n",
      "Epoch 209, Train Loss: 0.0021085349144414067\n",
      "Epoch 210, Train Loss: 0.0021266575204208493\n",
      "Epoch 211, Train Loss: 0.0021792741026729345\n",
      "Epoch 212, Train Loss: 0.00216784595977515\n",
      "Epoch 213, Train Loss: 0.002095532603561878\n",
      "Epoch 214, Train Loss: 0.002014793688431382\n",
      "Epoch 215, Train Loss: 0.0019696741946972907\n",
      "Epoch 216, Train Loss: 0.002025635214522481\n",
      "Epoch 217, Train Loss: 0.001983997761271894\n",
      "Epoch 218, Train Loss: 0.00199925119522959\n",
      "Epoch 219, Train Loss: 0.0020006379345431924\n",
      "Epoch 220, Train Loss: 0.0019492984865792096\n",
      "Epoch 221, Train Loss: 0.0020049322629347444\n",
      "Epoch 222, Train Loss: 0.002029338211286813\n",
      "Epoch 223, Train Loss: 0.0019250086043030024\n",
      "Epoch 224, Train Loss: 0.001901028968859464\n",
      "Epoch 225, Train Loss: 0.0019625479471869767\n",
      "Epoch 226, Train Loss: 0.0019942725775763392\n",
      "Epoch 227, Train Loss: 0.002011880395002663\n",
      "Epoch 228, Train Loss: 0.0019419672898948193\n",
      "Epoch 229, Train Loss: 0.0019773420644924045\n",
      "Epoch 230, Train Loss: 0.0018833558424375951\n",
      "Epoch 231, Train Loss: 0.0018337619840167463\n",
      "Epoch 232, Train Loss: 0.0018207937246188521\n",
      "Epoch 233, Train Loss: 0.0018416401580907404\n",
      "Epoch 234, Train Loss: 0.001876109279692173\n",
      "Epoch 235, Train Loss: 0.001788784225936979\n",
      "Epoch 236, Train Loss: 0.0017879331135191023\n",
      "Epoch 237, Train Loss: 0.0018223515362478793\n",
      "Epoch 238, Train Loss: 0.0018082884489558637\n",
      "Epoch 239, Train Loss: 0.0017710246029309928\n",
      "Epoch 240, Train Loss: 0.0018020993447862566\n",
      "Epoch 241, Train Loss: 0.0018342704279348254\n",
      "Epoch 242, Train Loss: 0.0017380912904627621\n",
      "Epoch 243, Train Loss: 0.0017333945725113153\n",
      "Epoch 244, Train Loss: 0.0017423684475943446\n",
      "Epoch 245, Train Loss: 0.002115782757755369\n",
      "Epoch 246, Train Loss: 0.0017495898064225912\n",
      "Epoch 247, Train Loss: 0.0020896601490676403\n",
      "Epoch 248, Train Loss: 0.002810928621329367\n",
      "Epoch 249, Train Loss: 0.0031962540233507752\n",
      "Epoch 250, Train Loss: 0.0019525075913406909\n",
      "Epoch 251, Train Loss: 0.0018433069344609976\n",
      "Epoch 252, Train Loss: 0.0023828628472983837\n",
      "Epoch 253, Train Loss: 0.0019701761193573475\n",
      "Epoch 254, Train Loss: 0.0020550870103761554\n",
      "Epoch 255, Train Loss: 0.0020336274756118655\n",
      "Epoch 256, Train Loss: 0.0028182048117741942\n",
      "Epoch 257, Train Loss: 0.0020045368000864983\n",
      "Epoch 258, Train Loss: 0.002074920805171132\n",
      "Epoch 259, Train Loss: 0.001810433401260525\n",
      "Epoch 260, Train Loss: 0.0018775343778543174\n",
      "Epoch 261, Train Loss: 0.001850405358709395\n",
      "Epoch 262, Train Loss: 0.0018329007434658706\n",
      "Epoch 263, Train Loss: 0.0017871642485260963\n",
      "Epoch 264, Train Loss: 0.001756263489369303\n",
      "Epoch 265, Train Loss: 0.001792294962797314\n",
      "Epoch 266, Train Loss: 0.0019253843929618597\n",
      "Epoch 267, Train Loss: 0.001703019777778536\n",
      "Epoch 268, Train Loss: 0.0017249550437554717\n",
      "Epoch 269, Train Loss: 0.001677840540651232\n",
      "Epoch 270, Train Loss: 0.0018789584282785654\n",
      "Epoch 271, Train Loss: 0.0018165603978559375\n",
      "Epoch 272, Train Loss: 0.001643062278162688\n",
      "Epoch 273, Train Loss: 0.001652355131227523\n",
      "Epoch 274, Train Loss: 0.0017102534766308963\n",
      "Epoch 275, Train Loss: 0.0016074590967036784\n",
      "Epoch 276, Train Loss: 0.0016960366629064083\n",
      "Epoch 277, Train Loss: 0.0024028756888583302\n",
      "Epoch 278, Train Loss: 0.002151538501493633\n",
      "Epoch 279, Train Loss: 0.0016953751328401268\n",
      "Epoch 280, Train Loss: 0.0016707712784409523\n",
      "Epoch 281, Train Loss: 0.0017027363064698875\n",
      "Epoch 282, Train Loss: 0.0016873524873517454\n",
      "Epoch 283, Train Loss: 0.0017142636352218688\n",
      "Epoch 284, Train Loss: 0.0017975559458136559\n",
      "Epoch 285, Train Loss: 0.0016513417940586805\n",
      "Epoch 286, Train Loss: 0.0015941709862090647\n",
      "Epoch 287, Train Loss: 0.0015434369561262429\n",
      "Epoch 288, Train Loss: 0.0015409409534186125\n",
      "Epoch 289, Train Loss: 0.0026024794206023216\n",
      "Epoch 290, Train Loss: 0.0015604239888489246\n",
      "Epoch 291, Train Loss: 0.0015194398001767695\n",
      "Epoch 292, Train Loss: 0.0016151167801581323\n",
      "Epoch 293, Train Loss: 0.0015419599367305636\n",
      "Epoch 294, Train Loss: 0.004884387482888997\n",
      "Epoch 295, Train Loss: 0.0016657192609272897\n",
      "Epoch 296, Train Loss: 0.012933854013681412\n",
      "Epoch 297, Train Loss: 0.009648874867707491\n",
      "Epoch 298, Train Loss: 0.022449722979217768\n",
      "Epoch 299, Train Loss: 0.02657558536157012\n",
      "Epoch 300, Train Loss: 0.014540134463459253\n",
      "Epoch 301, Train Loss: 0.015961484517902136\n",
      "Epoch 302, Train Loss: 0.01821584952995181\n",
      "Epoch 303, Train Loss: 0.014407075010240078\n",
      "Epoch 304, Train Loss: 0.050810196436941624\n",
      "Epoch 305, Train Loss: 0.02427596691995859\n",
      "Epoch 306, Train Loss: 0.017187952995300293\n",
      "Epoch 307, Train Loss: 0.011609320994466543\n",
      "Epoch 308, Train Loss: 0.010104747954756021\n",
      "Epoch 309, Train Loss: 0.016814426984637976\n",
      "Epoch 310, Train Loss: 0.007036508293822408\n",
      "Epoch 311, Train Loss: 0.006623255088925362\n",
      "Epoch 312, Train Loss: 0.004719105316326022\n",
      "Epoch 313, Train Loss: 0.004600136075168848\n",
      "Epoch 314, Train Loss: 0.004316346836276352\n",
      "Epoch 315, Train Loss: 0.003792685456573963\n",
      "Epoch 316, Train Loss: 0.004289205651730299\n",
      "Epoch 317, Train Loss: 0.0034349566558375955\n",
      "Epoch 318, Train Loss: 0.003631176892668009\n",
      "Epoch 319, Train Loss: 0.0031881676986813545\n",
      "Epoch 320, Train Loss: 0.003151782788336277\n",
      "Epoch 321, Train Loss: 0.0023325092624872923\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Entrenamiento\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10000\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 35\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer)\u001b[0m\n\u001b[1;32m     33\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     34\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 35\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Entrenamiento\n",
    "for epoch in range(10000):\n",
    "    train_loss = train(model, train_loader, optimizer)\n",
    "    print(f'Epoch {epoch + 1}, Train Loss: {train_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=34, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()  # Pon el modelo en modo evaluación\n",
    "model.to(device)  # Asegúrate de que el modelo esté en el dispositivo correcto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Asumiendo que 'MODEL_NAME' es el nombre del modelo BERT que usaste\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "text = \"\"\"\n",
    "Factura\n",
    "H748950\n",
    "\n",
    "Fecha de emisión: 2024-04-05\n",
    "Fecha de vencimiento: 2024-04-11\n",
    "\n",
    "Datos del Emisor:\n",
    "Hall, Howard and Compton\n",
    "\n",
    "Datos del Receptor:\n",
    "John Martin\n",
    "\n",
    "7780 Christine Underpass East Daniel, SD 10665 981 Perez Ports Cheryltown, VA 68274\n",
    "\n",
    "546-443-6969\n",
    "mathisnatalieOfitzgerald.com\n",
    "YzN809qpY325\n",
    "\n",
    "Cantidad Precio Unitario Total\n",
    "\n",
    "morph scalable functionalities\n",
    "harness viral eyeballs\n",
    "enable one-to-one systems\n",
    "\n",
    "iterate intuitive ROI\n",
    "\n",
    "(913)675-2764x8171\n",
    "tiffanys6O yahoo.com\n",
    "IKQ896GSX614\n",
    "\n",
    "4 34.04 136.16\n",
    "9 88.77 798.93\n",
    "5 60.5 302.5\n",
    "10 50.02 500.2\n",
    "\n",
    "Subtotal: 1737.79\n",
    "VAT (16%): 278.05\n",
    "Total: 2015.84\n",
    "\n",
    "Método de pago: Bank Transfer\n",
    "\"\"\"\n",
    "\n",
    "encoded_input = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "input_ids = encoded_input['input_ids'].to(device)\n",
    "attention_mask = encoded_input['attention_mask'].to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():  # No necesitas calcular gradientes aquí\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Aplicar softmax para obtener probabilidades\n",
    "probabilities = F.softmax(logits, dim=-1)\n",
    "predictions = torch.argmax(probabilities, dim=-1)\n",
    "predicted_labels = [label_encoder.inverse_transform([label.item()])[0] for label in predictions[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H748950 -> B-invoice_id\n",
      "2024-04-05 -> B-issue_date\n",
      "2024-04-11 -> B-due_date\n",
      "Hall, -> B-issuer_name\n",
      "HowardandCompton -> I-issuer_name\n",
      "John -> B-recipient_name\n",
      "Martin -> I-recipient_name\n",
      "7780 -> B-issuer_address\n",
      "ChristineUnderpassEastDaniel,SD10665 -> I-issuer_address\n",
      "981 -> B-recipient_address\n",
      "PerezPortsCheryltown,VA68274 -> I-recipient_address\n",
      "546-443-6969 -> B-issuer_phone\n",
      "mathisnatalieOfitzgerald.com -> B-issuer_email\n",
      "YzN809qpY325 -> B-issuer_tax_id\n",
      "morphscalablefunctionalitiesharnessviraleyeballs -> I-item_description\n",
      "enable -> B-item_description\n",
      "one-to-onesystems -> I-item_description\n",
      "iterate -> B-item_description\n",
      "intuitiveROI -> I-item_description\n",
      "(913)675-2764x8171 -> B-issuer_phone\n",
      "tiffanys6Oyahoo.com -> I-item_description\n",
      "IKQ896GSX614 -> B-recipient_tax_id\n",
      "434.04136.16988.77798.93560.5302 -> B-item_unit_price\n",
      ". -> B-item_total\n",
      "5 -> B-item_unit_price\n",
      "1050.02500.2 -> B-item_total\n",
      "1737.79 -> B-subtotal\n",
      "VAT -> B-tax_description\n",
      "(16%) -> B-tax_percentage\n",
      "278.05 -> B-tax_amount\n",
      "2015.84 -> B-total\n",
      "BankTransfer -> B-payment_method\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "\n",
    "#Compactar la salida tokens y etiquetas\n",
    "\n",
    "def compact_output(tokens, labels):\n",
    "    compacted_tokens = []\n",
    "    compacted_labels = []\n",
    "    current_token = ''\n",
    "    current_label = ''\n",
    "    for token, label in zip(tokens, labels):\n",
    "        if token.startswith('##'):\n",
    "            current_token += token[2:]\n",
    "        else:\n",
    "            if current_token:\n",
    "                compacted_tokens.append(current_token)\n",
    "                compacted_labels.append(current_label)\n",
    "            current_token = token\n",
    "            current_label = label\n",
    "    if current_token:\n",
    "        compacted_tokens.append(current_token)\n",
    "        compacted_labels.append(current_label)\n",
    "\n",
    "    #Eliminat todos los tokens que no son parte de ninguna entidad nombrada\n",
    "    for i in range(len(compacted_labels)):\n",
    "        if compacted_labels[i] == 'O':\n",
    "            compacted_tokens[i] = None\n",
    "            compacted_labels[i] = None\n",
    "        if compacted_labels[i] == 'UNK':\n",
    "            compacted_tokens[i] = None\n",
    "            compacted_labels[i] = None\n",
    "    \n",
    "    compacted_tokens = [token for token in compacted_tokens if token is not None]\n",
    "    compacted_labels = [label for label in compacted_labels if label is not None]\n",
    "            \n",
    "    \n",
    "    #Si tokens consecutivos tienen el mismo label, se concatenan\n",
    "    for i in range(1, len(compacted_labels)):\n",
    "        if compacted_labels[i] == compacted_labels[i - 1]:\n",
    "            compacted_tokens[i] = compacted_tokens[i - 1] + \" \" + compacted_tokens[i]\n",
    "            # Marcar el token anterior para eliminarlo\n",
    "            compacted_tokens[i - 1] = None\n",
    "            # Marcar el label anterior para eliminarlo\n",
    "            compacted_labels[i - 1] = None\n",
    "\n",
    "    compacted_tokens = [token for token in compacted_tokens if token is not None]\n",
    "    compacted_labels = [label for label in compacted_labels if label is not None]\n",
    "    \n",
    "    return compacted_tokens, compacted_labels\n",
    "\n",
    "\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "tokens, labels = compact_output(tokens, predicted_labels)\n",
    "for token, label in zip(tokens, labels):\n",
    "    print(f'{token} -> {label}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
